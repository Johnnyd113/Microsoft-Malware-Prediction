import os
import json
import path
import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import KFold
import time
np.random.seed(11)

#Assign all directories from SETTINGS.json
with open('SETTINGS.json', 'r') as f:
    config = json.load(f)
train_path = config['TRAIN_DATA_CLEAN_PATH']
models_dir = config['MODELS_DIR']
feature_importances_path = config['FEATURE_IMPORTANCE_PATH']

train = pd.read_pickle(train_path)

kf = KFold(n_splits=5, shuffle=True, random_state=11)

X = train.drop(columns=['MachineIdentifier','HasDetections','test_set'])
y = train['HasDetections']

#Based on https://www.kaggle.com/artgor/is-this-malware-eda-fe-and-lgb-updated
#Adjusted num_leaves and max_septh
params = {'num_leaves': 128,
         'min_data_in_leaf': 42,
         'objective': 'binary',
         'metric': 'auc',
         'max_depth': -1,
         'learning_rate': 0.05,
         'num_threads': 18,
         "boosting": "gbdt",
         "feature_fraction": 0.8,
         "bagging_freq": 5,
         "bagging_fraction": 0.8,
         "bagging_seed": 11,
         "lambda_l1": 0.15,
         "lambda_l2": 0.15,
         "random_state": 42}

#Create a dataframe to store feature importances during training
feats = pd.DataFrame()
feats['feature'] = X.columns
feats['importance_split'] = 0
feats['importance_gain'] = 0

start_time = time.time()

for i, (train_ind, val_ind) in enumerate(kf.split(train)):
    
    print(f'Beginning fold {i}')
    
    train_data = lgb.Dataset(X.iloc[train_ind],
                           label=y.iloc[train_ind])

    val_data = lgb.Dataset(X.iloc[val_ind],
                           label=y.iloc[val_ind])


    model = lgb.train(params,
                      train_data,
                      num_boost_round=10000,
                      valid_sets = [train_data, val_data],
                      verbose_eval=100,
                      early_stopping_rounds = 100)
    
    #Save both split and gain importances, averaged over 5 folds
    feats['importance_split'] += model.feature_importance()/5
    feats['importance_gain'] += model.feature_importance(importance_type='gain')/5
    
    model.save_model(f'{models_dir}model_{i}',num_iteration=model.best_iteration)

end_time = time.time()
run_time = end_time - start_time
print(f'Time to train: {run_time}')

feats.to_csv(feature_importances_path, index=False)
