import os
import json
import numpy as np
import pandas as pd

with open('SETTINGS.json', 'r') as f:
    config = json.load(f)

raw_data = config['RAW_DATA_DIR']
train_clean= config['TRAIN_DATA_CLEAN_PATH']
test_clean = config['TEST_DATA_CLEAN_PATH']

#Function to reduce file mem size
#Based on https://www.kaggle.com/gemartin/load-data-reduce-memory-usage
def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage(deep=True).sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage(deep=True).sum() / 1024**2
    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df


train = reduce_mem_usage(pd.read_csv(f'{raw_data}train.csv'), verbose=False)
test = reduce_mem_usage(pd.read_csv(f'{raw_data}test.csv'), verbose=False)

#ID train v test data
train['test_set'] = 0
test['test_set'] = 1

full = pd.concat([train,test],ignore_index=True,sort=False)

del train
del test

#Replace strings with count encodings over the combined train and test data
for c in full.drop(columns=['MachineIdentifier','HasDetections','test_set']).columns:
    if full[c].dtype == object:
        print(f'converted {c}')
        full[c] = full[c].map(full[c].value_counts())
        
full = reduce_mem_usage(full, verbose=False)

train = full[full['test_set']==0]
test = full[full['test_set']==1]

train.to_pickle(train_clean)
test.to_pickle(test_clean)

print('data saved')
